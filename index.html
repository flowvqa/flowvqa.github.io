<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>FlowVQA</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=DM+Sans:opsz,wght@1,9..40,900&display=swap" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name" style="font-size: 75px;">FlowVQA ðŸŒŠðŸ¤”ðŸ’¡</h1>
      <h2 class="project-tagline" style="font-weight: bolder; font-size: 23px;"> Mapping Multimodal Logic in
        Visual Question Answering with Flowcharts</h2>
        <a href="https://arxiv.org/abs/2406.19237" class="btn">Paper</a>
        <a href="https://github.com/flowvqa/flowvqa" target="_blank" class="btn">Dataset</a>
        <a href="https://flowchart-qa-verification.vercel.app/" target="_blank" class="btn">Verification Platform</a>
        <a href="https://github.com/PurviChaurasia/flowchart-qa-verification" target="_blank" class="btn">Platform code</a>
    </section>

    <section class="main-content">
      <h1>
        <a id="user-content-header-1" class="anchor" href="#header-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>FlowVQA: Mapping Multimodal Logic in
        Visual Question Answering with Flowcharts</h1>
      
        <h2>
          <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>About</h2>

      <p>Current benchmarks for visual question answering fall short in evaluating crucial aspects 
        like visual grounding and spatial reasoning skills. We introduce FlowVQA, a novel benchmark 
        aimed at assessing the capabilities of visual question-answering MLLMs 
        with flowcharts as visual contexts. This innovative benchmark brings together 
        2,272 carefully generated flowchart images and 22,413 question-answer pairs, challenging 
        multimodal language models with tasks like information localization, decision-making, and 
        logical progression. Our findings underscore the limitations of State-of-the-Art models across
         different categories in our dataset, highlighting the benchmark's crucial role in advancing 
         multimodal question-answer modeling.</p>

      <h2>
        <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset</h2>
        <p>We collect input texts from three primary sources: <strong>Wikihow</strong> articles, <strong>Instructables</strong> DIY 
          blogs, and <strong>FloCo</strong> code snippets. WikiHow and Instructables provide detailed instructions for 
          various tasks, while FloCo, a resource converting flowcharts to code, contains simple code 
          samples. For each flowchart, we generate questions across 4 categories - <strong>Fact Retreival</strong>, 
          <strong>Applied Scenraio</strong>, <strong>Flow Referential</strong> and <strong>Toplogical</strong>, to test different aspects of MLLMs.</p>
        <p>Our final dataset includes <strong>1,121</strong> WikiHow articles, <strong>701</strong> Instructables blogs, and <strong>450</strong> FloCo flowcharts 
          along with a total of <strong>22,413</strong> diverse question-answer pairs.</p>
        <img src="figures/Dataset_distribution.png" alt="Flowchart Generation Pipeline"style="width: 70%; margin: 0 auto; display: block;">

      <h2>
        <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Flowchart Generation</h2>
        <img src="figures\Flowchart Generation (3).png" alt="Flowchart Generation Pipeline" style="margin-left: -90px;">
        <p>
          Our core approach centers on converting <em>any process-based workflow, regardless of its domain,
           into a flowchart for a detailed step-by-step representation</em>. The conversion from source 
           articles to flowchart Mermaid Scripts involves a two-step process, as shown in the figure. 
           <!-- Initially, GPT-4 generates 
           a structured representation with functional control tags from the source text, transforming 
           it into a tagged textual representation suitable for Mermaid flowchart scripts. In the second 
           step, we use this output to create a top-down Mermaid.js flowchart script, mapping steps to 
           node types with control tags. The resulting scripts are compiled into high-resolution PNG images. --></p>
      <h2>
        <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Question Generation</h2>
        <img src="figures/QA_generation.png" alt="Question Generation Pipeline" width="1000px" height="auto">
        <p>Our Q/A creation process encompasses four distinct question types: Fact Retrieval, Applied 
          Scenario, Flow Referential, and Topological Q/A. 
          <!-- Fact Retrieval involves extracting factual 
          information from flowchart nodes, Applied Scenario tests practical application skills, Flow 
          Referential focuses on specific sub-graphs, and Topological Q/A explores the larger flowchart 
          structure. --> To generate high-quality Q/A pairs, we query GPT-4 using tagged textual representation, 
          Mermaid.js script, and text-only few-shot examples. Topological Q/A pairs are produced by parsing 
          the Mermaid script and creating adjacnecy matrices from them. 
          <!-- The questions are template based 
          and quantitative answers are extracted using networkx. We ensure evaluation consistency by generating 
          three paraphrased gold answers for each question type, accommodating syntactic and semantic 
          variations.--> </p>
            
          <h2>
            <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Example
          </h2>
          <div style="display: flex; justify-content: flex-end; align-items: center;">
            <div>
              <p><strong>Fact Retrieval</strong></p>
              <ul>
                <li><strong>Q:</strong>What triggers the LED to light up?</li>
                <li><strong>A:</strong>Receiving a text or call</li>
              </ul>
              <p><strong>Flow Referential</strong></p>
              <ul>
                <li><strong>Q:</strong> Suppose the LED has just lit up by the LDR. What was the immediate previous step, and what decision do I need to make next?</li>
                <li><strong>A:</strong> The immediate previous step was waiting for a text or call, and the next decision is to determine if the LDR value is indicating a command.</li>
              </ul>
              <p><strong>Applied Scenario</strong></p>
              <ul>
                <li><strong>Q:</strong> Jasmine has just received a picture message on her working cellphone, which was sent from her DIY surveillance system using the modified Sony Ericsson T630. Prior to this, what sequence of actions did the system perform to capture and send the picture to Jasmine?</li>
                <li><strong>A:</strong> The system executed the 'run' command sequence to navigate the phone's menu, took a picture, attached it to a message, and sent it to Jasmine.</li>
              </ul>
              <p><strong>Topological</strong></p>
              <ul>
                <li><strong>Q:</strong> How many nodes exist in the given flowchart?</li>
                <li><strong>A:</strong> 22</li>
              </ul>
              <p>Hover on the image to expand it.</p>
            </div>
            <img src="figures/instruct00094.png" alt="Example image from instructables" width="200px" height="auto" style="margin-left: 20px;" onmouseenter="enlargeImg()" onmouseleave="resetImg()" id="img1">
          </div>
          
      <!-- script to set display property -->
    <script>
      // Get the img object using its Id
      img = document.getElementById("img1");
      // Function to increase image size
      function enlargeImg() {
          // Set image size to 1.5 times original
          img.style.transform = "scale(2)";
          // Animation effect
          img.style.transition = "transform 0.25s ease";
      }
      // Function to reset image size
      function resetImg() {
          // Set image size to original
          img.style.transform = "scale(1)";
          img.style.transition = "transform 0.25s ease";
      }
  </script>
        <h2>
          <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimental Results</h2>
          <img src="figures/ExperimentalResult.png" alt="Experimental Results on the test set of FlowVQA" width="900px" height="auto">
      <blockquote>
      <p>We perform evaluation on 3 different strategies: <strong>Zero Shot</strong>, <strong>Zero Shot with Chain-of-thought prompting</strong> and <strong>Few Shot Chain-of-thought prompting with Reasong Directives</strong>. The last strategy is our novel approach for decomposing the flowchart for better QA performance.</p>
      </blockquote>
      <p>FlowVQA poses a considerable challenge for models, evident in evaluations highlighting opportunities 
        for improvement. The leading strategy, GPT-4 with Few-shot directive-based prompting, achieves 
        a notable 68.42% Majority Voting score. The Qwen-VL-chat fine-tuned model surpasses all existing open-source models, 
        underscoring the importance of fine-tuning for addressing flowchart understanding and emphasizing FlowVQA's potential in introducing visual logic and reasoning to MLLMs.</p>

      
      <h2>
        <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>People</h2>
        <p style="text-align: justify;"> The FlowVQA dataset is prepared by the following people: </p>
        <figure style="display: flex; flex-wrap: wrap; justify-content: center;">
          <a href="https://www.linkedin.com/in/koseii/">
            <img src="figures/Shubhankar.jpg" style="width:150px; height: 150px; margin: 10px;">
            <figcaption>Shubhankar Singh</figcaption>
        </a>
        <a href="https://www.linkedin.com/in/purvi-chaurasia/">
          <img src="figures/purvi.jpeg" style="width:150px; height: 150px; margin: 10px;">
          <figcaption>Purvi Chaurasia</figcaption>
      </a>
      <a href="https://www.linkedin.com/in/yvarun25221/">
        <img src="figures/varun.jpg" style="width:150px; height: 150px; margin: 10px;">
        <figcaption>Yerram Varun</figcaption>
    </a>
    <a href="https://www.linkedin.com/in/pranshu-pandya/">
      <img src="figures/Pranshu.jpg" style="width:150px; height: 150px; margin: 10px;">
      <figcaption>Pranshu Pandya</figcaption>
  </a>
  <a href="https://www.linkedin.com/in/vatsal-gupta-iitg/">
    <img src="figures/vatsal.jpg" style="width:150px; height: 150px; margin: 10px;">
    <figcaption>Vatsal Gupta</figcaption>
</a>
<a href="https://vgupta123.github.io">
  <img src="figures/vivek.jpeg" style="width:150px; height: 150px; margin: 10px;">
  <figcaption>Vivek Gupta</figcaption>
</a>
<a href="https://www.cis.upenn.edu/~danroth/">
  <img src="figures/dan.jpg" style="width:150px; height: 150px; margin: 10px;">
  <figcaption>Dan Roth</figcaption>
</a>
      </figure> 
      We are also immensely grateful for the support of our fellow annotators in the creation of the dataset - Annie Mathew, Anushka Singh, Saumya Gupta, Pallaprolu Pragnesh Kumar, Shriya Sandilya.
      
      
      <h2>
      <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Citation</h3>
      <p>Please cite our paper as below if you use the FLOWVQA dataset.</p>
      <pre><code>@inproceedings{singh-etal-2024-flowvqa,
    title = "{F}low{VQA}: Mapping Multimodal Logic in Visual Question Answering with Flowcharts",
    author = "Singh, Shubhankar  and
      Chaurasia, Purvi  and
      Varun, Yerram  and
      Pandya, Pranshu  and
      Gupta, Vatsal  and
      Gupta, Vivek  and
      Roth, Dan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.78",
    doi = "10.18653/v1/2024.findings-acl.78",
    pages = "1330--1350",
    abstract = "Existing benchmarks for visual question answering lack in visual grounding and complexity, particularly in evaluating spatial reasoning skills. We introduce FlowVQA, a novel benchmark aimed at assessing the capabilities of visual question-answering multimodal language models in reasoning with flowcharts as visual contexts. FlowVQA comprises 2,272 carefully generated and human-verified flowchart images from three distinct content sources, along with 22,413 diverse question-answer pairs, to test a spectrum of reasoning tasks, including information localization, decision-making, and logical progression. We conduct a thorough baseline evaluation on a suite of both open-source and proprietary multimodal language models using various strategies, followed by an analysis of directional bias. The results underscore the benchmark{'}s potential as a vital tool for advancing the field of multimodal modeling, providing a focused and challenging environment for enhancing model performance in visual and logical reasoning tasks.",
}</code></pre>
      <footer class="site-footer">
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman</a> theme by <a href="https://github.com/jasonlong">jasonlong</a>.</span>
      </footer>

    </section>

  </body>
</html>
