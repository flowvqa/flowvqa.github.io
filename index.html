<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>FlowVQA</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link rel="stylesheet" href="css/normalize.css">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="css/cayman.css">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">FlowVQA</h1>
      <h2 class="project-tagline"> Mapping Multimodal Logic in
        Visual Question Answering with Flowcharts</h2>
      <a href="#" class="btn">Paper</a>
      <a href="#" class="btn">Dataset</a>
      <a href="#" class="btn">Explore</a>
      <a href="#" class="btn">Code</a>
    </section>

    <section class="main-content">
      <h1>
        <a id="user-content-header-1" class="anchor" href="#header-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>FlowVQA: Mapping Multimodal Logic in
        Visual Question Answering with Flowcharts</h1>
      
        <h2>
          <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>About</h2>

      <p>Current benchmarks for visual question answering fall short in evaluating crucial aspects 
        like visual grounding and spatial reasoning skills. We introduce FlowVQA, a novel benchmark 
        aimed at assessing the capabilities of visual question-answering multimodal language models 
        in reasoning with flowcharts as visual contexts. This innovative benchmark brings together 
        2,272 carefully generated flowchart images and 22,413 question-answer pairs, challenging 
        multimodal language models with tasks like information localization, decision-making, and 
        logical progression. Our findings underscore the limitations of State-of-the-Art models across
         different categories in our dataset, highlighting the benchmark's crucial role in advancing 
         multimodal question-answer modeling.</p>

      <h2>
        <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset</h2>
        <p>We collect input texts from three primary sources: <strong>Wikihow</strong> articles, <strong>Instructables</strong> DIY 
          blogs, and <strong>FloCo</strong> code snippets. WikiHow and Instructables provide detailed instructions for 
          various tasks, while FloCo, a resource converting flowcharts to code, contains simple code 
          samples. For each flowchart, we generate questions across 4 categories - <strong>Fact Retreival</strong>, 
          <strong>Applied Scenraio</strong>, <strong>Flow Referential</strong> and <strong>Toplogical</strong>, to test different aspects of MLLMs.</p>
        <p>Our final dataset includes <strong>1,121</strong> WikiHow articles, <strong>701</strong> Instructables blogs, and <strong>450</strong> FloCo flowcharts 
          along with a total of <strong>22,413</strong> diverse question-answer pairs.</p>
        <img src="figures/Dataset_distribution.png" alt="Flowchart Generation Pipeline"style="width: 70%; margin: 0 auto; display: block;">

      <h2>
        <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Flowchart Generation</h2>
        <img src="figures/Flowchart_generation.png" alt="Flowchart Generation Pipeline">
        <p>
          Our core approach centers on converting <em>any process-based workflow, regardless of its domain,
           into a flowchart for a detailed step-by-step representation</em>. The conversion from source 
           articles to flowchart Mermaid Scripts involves a two-step process. Initially, GPT-4 generates 
           a structured representation with functional control tags from the source text, transforming 
           it into a tagged textual representation suitable for Mermaid flowchart scripts. In the second 
           step, we use this output to create a top-down Mermaid.js flowchart script, mapping steps to 
           node types with control tags. The resulting scripts are compiled into high-resolution PNG images.</p>
      <h2>
        <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Question Generation</h2>
        <img src="figures/QA_generation.png" alt="Question Generation Pipeline">
        <p>Our Q/A creation process encompasses four distinct question types: Fact Retrieval, Applied 
          Scenario, Flow Referential, and Topological Q/A. Fact Retrieval involves extracting factual 
          information from flowchart nodes, Applied Scenario tests practical application skills, Flow 
          Referential focuses on specific sub-graphs, and Topological Q/A explores the larger flowchart 
          structure. To generate high-quality Q/A pairs, we query GPT-4 using tagged textual representation, 
          Mermaid.js script, and text-only few-shot examples. Topological Q/A pairs are produced by parsing 
          the Mermaid script and creating adjacnecy matrices from them. The questions are template based 
          and quantitative answers are extracted using networkx. We ensure evaluation consistency by generating 
          three paraphrased gold answers for each question type, accommodating syntactic and semantic 
          variations. </p>
      <h2>
        <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Example</h2>
      <p>Examples from our dataset can be viewed <a href="https://purvichaurasia.notion.site/FlowVQA-Examples-61fa4e22d1b74ec28305438c3a24b86f?pvs=4">here</a>.</p>
        <h2>
          <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimental Results</h2>
          <img src="figures/ExperimentalResult.png" alt="Experimental Results on the test set of FlowVQA">
      <blockquote>
      <p>We perform evaluation on 3 different strategies: <strong>Zero Shot</strong>, <strong>Zero Shot with Chain-of-thought prompting</strong> and <strong>Few Shot Chain-of-thought prompting with Reasong Directives</strong>. The last strategy is our novel approach for decomposing the flowchart for better QA performance. We have fine-tuned (FT) the Qwen-VL Chat model as well.</p>
      </blockquote>
      <p>FlowVQA poses a considerable challenge for models, evident in evaluations highlighting opportunities 
        for improvement. The leading strategy, GPT-4 with Few-shot directive-based prompting, achieves 
        a notable 68.42% Majority Voting score. Few-Shot Directives prove effective, showing a considerable
        improvement in most models. Fine-tuning Qwen-VL-chat with 
        Zero-Shot and Zero-Shot CoT strategies results in a 3% and 11% improvement, respectively, 
        underscoring the importance of fine-tuning for addressing flowchart understanding limitations 
        in original pretraining. The fine-tuned model surpasses all existing open-source models, 
        emphasizing FlowVQA's potential in introducing visual logic and reasoning to MLLMs.</p>

      <h2>
        <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>People</h2>
      <h2>
      <a id="user-content-header-2" class="anchor" href="#header-2" aria-hidden="true"><span class="octicon octicon-link"></span></a>Citation</h3>
      <pre><code>This is a code block following a header.</code></pre>
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/jasonlong/cayman-theme">Cayman</a> is maintained by <a href="https://github.com/jasonlong">jasonlong</a>.</span>
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>

    </section>

  </body>
</html>
